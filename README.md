## REFERENCES

1. Hyperparameters Optimization:
	* https://en.wikipedia.org/wiki/Hyperparameter_optimization
	* https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide

2. Network Architecture Search: 
	* https://en.wikipedia.org/wiki/Neural_architecture_search
	* https://www.automl.org/nas-overview/

3. Bayesan Optimization:
	* https://research.facebook.com/blog/2021/07/optimizing-model-accuracy-and-latency-using-bayesian-multi-objective-neural-architecture-search/

4. Tools
	* AX: https://ax.dev/docs/why-ax.html


## TO DO

1. State-of-the-art review: start to write the first part of the thesis:
	* Overview of hyperparameter optimization, NAS, meta-learning etc.
	* Overview of the methods: how they work? Which are the PROs? Which are the CONs?
	* Available tools

2. Describe the constraints for the optimization:
	* Latency
	* Power
	* Area
	* Architecture: maximum number of neurons, synapses
	* Bit-width
	
	How can we describe them? How can we formalize them? Should we consider quantization at this level?

3. Keep track of whatever you think can be useful, like tools, papers, websites, books, blogs, whichever thing, here. Keep this readme updated.
